{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9d9d786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Lecture du CV via OCR...\n",
      "üåê Traduction en anglais...\n",
      "üìä Calcul du matching...\n",
      "\n",
      "üîç Matching Percentage: 21.41%\n",
      "\n",
      "üìå Matching Keywords:\n",
      "       Term  Avg_TFIDF_Weight\n",
      "       data          0.366929\n",
      "   learning          0.148246\n",
      "    machine          0.148246\n",
      " algorithms          0.117122\n",
      "engineering          0.117122\n",
      " experience          0.117122\n",
      "     python          0.117122\n",
      "   analysis          0.101561\n",
      "   building          0.101561\n",
      " predictive          0.101561\n"
     ]
    }
   ],
   "source": [
    "# üì¶ IMPORTS\n",
    "import fitz  # PyMuPDF\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import io\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "from googletrans import Translator\n",
    "\n",
    "# üîß Patch asyncio pour environnement interactif (Jupyter, VSCode interactive, etc.)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# üõ† Configurer Tesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# üìÑ Chemin vers le PDF du CV\n",
    "resume_path = r\"C:\\Users\\khmir\\Desktop\\cvs\\khmiri_iheb_tun_fr.pdf\"\n",
    "\n",
    "# üîç Description de poste (en anglais)\n",
    "job_description = \"\"\"\n",
    "We are looking for a Data Scientist intern who has experience with Python, machine learning algorithms, and data analysis. \n",
    "The candidate will be responsible for data cleaning, feature engineering, and building predictive models.\n",
    "\"\"\"\n",
    "\n",
    "# üìå Fonction : extraire le texte OCR du PDF (en fran√ßais)\n",
    "def extract_text_from_pdf_ocr(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    full_text = \"\"\n",
    "    for page in doc:\n",
    "        pix = page.get_pixmap(dpi=300)\n",
    "        img = Image.open(io.BytesIO(pix.tobytes(\"png\")))\n",
    "        text = pytesseract.image_to_string(img, lang=\"fra\")\n",
    "        full_text += text + \"\\n\"\n",
    "    return full_text\n",
    "\n",
    "# üìå Fonction principale async (traduction + matching)\n",
    "async def main():\n",
    "    # 1. OCR\n",
    "    print(\"‚è≥ Lecture du CV via OCR...\")\n",
    "    cv_text_fr = extract_text_from_pdf_ocr(resume_path)\n",
    "\n",
    "    # 2. Traduction\n",
    "    print(\"üåê Traduction en anglais...\")\n",
    "    translator = Translator()\n",
    "    translation = await translator.translate(cv_text_fr, src='fr', dest='en')\n",
    "    cv_text_en = translation.text\n",
    "\n",
    "    # 3. TF-IDF & Similarit√©\n",
    "    print(\"üìä Calcul du matching...\")\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform([cv_text_en, job_description])\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "    matching_percentage = cosine_sim[0][0] * 100\n",
    "\n",
    "    print(f\"\\nüîç Matching Percentage: {matching_percentage:.2f}%\\n\")\n",
    "\n",
    "    # 4. Analyse des mots-cl√©s\n",
    "    cv_vector = tfidf_matrix[0].toarray()[0]\n",
    "    job_vector = tfidf_matrix[1].toarray()[0]\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    common_terms = []\n",
    "    for idx, term in enumerate(feature_names):\n",
    "        if cv_vector[idx] > 0 and job_vector[idx] > 0:\n",
    "            avg_score = (cv_vector[idx] + job_vector[idx]) / 2\n",
    "            common_terms.append((term, avg_score))\n",
    "\n",
    "    # Trier et afficher\n",
    "    common_terms_sorted = sorted(common_terms, key=lambda x: x[1], reverse=True)\n",
    "    df = pd.DataFrame(common_terms_sorted, columns=[\"Term\", \"Avg_TFIDF_Weight\"])\n",
    "    print(\"üìå Matching Keywords:\")\n",
    "    print(df.head(100).to_string(index=False))\n",
    "\n",
    "# ‚ñ∂Ô∏è Lancer la fonction dans un environnement interactif\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24c87e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading full_matcher ...\n",
      "loading abv_matcher ...\n",
      "loading full_uni_matcher ...\n",
      "loading low_form_matcher ...\n",
      "loading token_matcher ...\n",
      "- * Design, testing, and integration of dashboards into a web applica-\n",
      "\n",
      "- * Development of an intelligent system to extract key information\n",
      "\n",
      "- * Design of a chatbot connected to the Data Warehouse for interac-\n",
      "\n",
      "- Technologies: Python, NLP, Airflow, Microsoft Power BI, MongoDB,\n",
      "\n",
      "- Technologies: Python, EDA, Airflow, MongoDB, PostgreSQL, Docker,\n",
      "\n",
      "- * Data collection and preprocessing through web scraping.\n",
      "\n",
      "- * Audit, validation, and deployment of the dashboard to a website.\n",
      "\n",
      "- Technologies: Python, EDA, SQL Workbench, Talend, Microsoft\n",
      "\n",
      "[INFO] Dates d√©tect√©es : February 2025 - August 2025\n",
      "[INFO] Dates d√©tect√©es : June 2024 - August 2024\n",
      "[INFO] Dates d√©tect√©es : January 2024 ‚Äî June 2024\n",
      "[INFO] Dates d√©tect√©es : September 2020 - Present\n",
      "[INFO] Dates d√©tect√©es : September 2019 - July 2020\n",
      "\n",
      "========== R√©sultats de l'analyse du CV ==========\n",
      "Comp√©tences d√©tect√©es: intelligent system, audit, angular, sql, tunis, power bi, devops, mongodb, dashboards, business technologies, validation, machine learning, web development, scrum, design testing, data integration, etl, data collection, django, dashboard, anomaly detection, english, predictive maintenance, innovative, python, french, big datum, business intelligence, professional, sonarqube, net core, need analysis, web scraping, computer science, junit, data warehouse, eda, computer engineering, chatbot, grafana, next js, spring boot, airflow, mockito, machine learn, ssis, data analysis, workbench, talend, system design, arabic, prometheus, integration, elasticsearch, react js, algorithms, docker, transformers, microsoft azure, postgresql\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khmir\\AppData\\Local\\Temp\\ipykernel_15812\\2581602973.py:114: DeprecationWarning: Parsing dates involving a day of month without a year specified is ambiguious\n",
      "and fails to parse leap day. The default behavior will change in Python 3.15\n",
      "to either always raise an exception or to use a different default year (TBD).\n",
      "To avoid trouble, add a specific year to the input & format.\n",
      "See https://github.com/python/cpython/issues/70647.\n",
      "  parsed_date = search_dates(line, languages=[\"fr\", \"en\"])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from skillNer.skill_extractor_class import SkillExtractor\n",
    "from dateparser.search import search_dates\n",
    "from datetime import datetime\n",
    "from langdetect import detect\n",
    "import geonamescache\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import warnings\n",
    "import joblib\n",
    "import json\n",
    "import random\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import os\n",
    "\n",
    "# ========== Initialisation ==========\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "gc = geonamescache.GeonamesCache()\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# === Base de comp√©tences\n",
    "SKILL_DB_PATH = r\"C:\\Users\\khmir\\Desktop\\data-science\\skill_db_relax_20.json\"\n",
    "with open(SKILL_DB_PATH, 'r', encoding='utf-8') as f:\n",
    "    SKILL_DB = json.load(f)\n",
    "\n",
    "skill_extractor = SkillExtractor(nlp, SKILL_DB, PhraseMatcher)\n",
    "\n",
    "# ========== Fonctions NLP de base ==========\n",
    "def count_verbs(sentence): return len([t for t in nlp(sentence) if t.pos_ == 'VERB'])\n",
    "def count_adjectives(sentence): return len([t for t in nlp(sentence) if t.pos_ == 'ADJ'])\n",
    "def count_stopwords(sentence): return len([t for t in nlp(sentence) if t.is_stop])\n",
    "def count_nouns(sentence): return len([t for t in nlp(sentence) if t.pos_ == 'NOUN'])\n",
    "def count_digits(sentence): return len([t for t in nlp(sentence) if t.is_digit])\n",
    "def count_special_characters(sentence): return len([t for t in nlp(sentence) if not t.text.isalnum() and not t.is_punct])\n",
    "def count_punctuation(sentence): return len([t for t in nlp(sentence) if t.is_punct])\n",
    "def calculate_sentence_length(sentence): return len(nlp(sentence))\n",
    "\n",
    "# ========== OCR Extraction PDF ==========\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    images = convert_from_path(pdf_path)\n",
    "    all_text = \"\"\n",
    "    for image in images:\n",
    "        all_text += pytesseract.image_to_string(image, lang='eng+fra') + \"\\n\"\n",
    "    temp_txt_path = pdf_path.replace('.pdf', '.txt')\n",
    "    with open(temp_txt_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(all_text)\n",
    "    return temp_txt_path\n",
    "\n",
    "# ========== D√©tection des comp√©tences ==========\n",
    "def extract_skills(skill_extractor, sentence):\n",
    "    try:\n",
    "        annotations = skill_extractor.annotate(sentence)\n",
    "        unique_values = set()\n",
    "        for item in annotations['results']['full_matches']:\n",
    "            unique_values.add(item['doc_node_value'].lower())\n",
    "        for item in annotations['results']['ngram_scored']:\n",
    "            unique_values.add(item['doc_node_value'].lower())\n",
    "        return list(unique_values)\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur comp√©tences: {e}\")\n",
    "        return []\n",
    "\n",
    "def count_skills(skill_extractor, sentence):\n",
    "    return len(extract_skills(skill_extractor, sentence))\n",
    "\n",
    "def detectSkills(skill_extractor, file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        cc = f.read()\n",
    "    annotations = skill_extractor.annotate(cc)\n",
    "    unique_values = set()\n",
    "    for item in annotations['results']['full_matches']:\n",
    "        skill = item['doc_node_value'].lower()\n",
    "        unique_values.add(' '.join(dict.fromkeys(skill.split())))\n",
    "    for item in annotations['results']['ngram_scored']:\n",
    "        skill = item['doc_node_value'].lower()\n",
    "        unique_values.add(' '.join(dict.fromkeys(skill.split())))\n",
    "    unique_values.discard('')\n",
    "    return list(unique_values)\n",
    "\n",
    "# ========== Exp√©rience et nettoyage ==========\n",
    "def calculate_total_years_experience(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    lines = content.splitlines()\n",
    "    total_years = 0\n",
    "    printed_lines = set()\n",
    "\n",
    "    for line in lines:\n",
    "        original_line = line\n",
    "        line = line.lower()\n",
    "        line = re.sub(r\"\\b(months?|years?|mos|yr|yrs|mois|an|ans)\\b\", \"\", line, flags=re.IGNORECASE)\n",
    "        line = line.replace(\".\", \"\").replace(\"/\", \" \").replace(\"-\", \" \")\n",
    "        for kw in [\"present\", \"today\", \"now\", \"aujourd'hui\"]:\n",
    "            line = line.replace(kw, datetime.now().strftime(\"%b %d, %Y\"))\n",
    "\n",
    "        parsed_date = search_dates(line, languages=[\"fr\", \"en\"])\n",
    "        if parsed_date:\n",
    "            parsed_dates = [date[1] for date in parsed_date]\n",
    "            if len(parsed_dates) >= 2:\n",
    "                parsed_dates.sort()\n",
    "                date1, date2 = parsed_dates[:2]\n",
    "                diff_years = (date2.year - date1.year) + (date2.month - date1.month) / 12.0\n",
    "                total_years += diff_years\n",
    "                printed_lines.add(original_line)\n",
    "                print(f\"[INFO] Dates d√©tect√©es : {original_line}\")\n",
    "    return round(total_years, 2)\n",
    "\n",
    "def detect_location(text, locations):\n",
    "    return [loc for loc in locations if re.search(r'\\b' + re.escape(loc) + r'\\b', text, re.IGNORECASE)]\n",
    "\n",
    "def detect_address(file_path):\n",
    "    countries = [country['name'] for country in gc.get_countries().values()]\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    return detect_location(content, countries)\n",
    "\n",
    "def clean_ref(ref):\n",
    "    cleaned_ref = re.sub(r'\\[|\\]|\\s+|DCE', '', ref, flags=re.IGNORECASE)\n",
    "    match = re.search(r'Ref(\\d+)', cleaned_ref, re.IGNORECASE)\n",
    "    return f\"ref{match.group(1)}\" if match else \"ref not found\"\n",
    "\n",
    "# ========== Machine Learning ==========\n",
    "def train_dataset():\n",
    "    dataset = pd.read_excel('dataset_final.xlsx')\n",
    "    dataset = dataset.drop(dataset[(dataset['IsExperience'] == 'YES') & ((dataset['Sentence length'] < 3) | (dataset['Sentence length'] > 28))].index)\n",
    "    dataset = dataset.drop(dataset[(dataset['IsExperience'] == 'YES') & (dataset['experiences'].str.contains(\"\\\\?\"))].index)\n",
    "\n",
    "    numeric_features = ['Verbs number', 'Adjectives number', 'Stopwords number', 'Sentence length', 'Nouns number', 'Special chars number', 'Punctuation number', 'Digits number', 'Skills number']\n",
    "    numeric_transformer = Pipeline([('scaler', StandardScaler()), ('pca', PCA(n_components=2))])\n",
    "    categorical_transformer = Pipeline([('imputer', SimpleImputer(strategy='constant', fill_value='missing')), ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "    global preprocessor\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, ['experiences'])\n",
    "    ])\n",
    "\n",
    "    X = dataset.drop('IsExperience', axis=1)\n",
    "    y = LabelEncoder().fit_transform(dataset['IsExperience'])\n",
    "    X_transformed = preprocessor.fit_transform(X)\n",
    "\n",
    "    classifier = RandomForestClassifier()\n",
    "    classifier.fit(X_transformed, y)\n",
    "    joblib.dump(classifier, 'random_forest_model.pkl')\n",
    "    joblib.dump(preprocessor, 'preprocessor.pkl')\n",
    "\n",
    "def predict(filepath):\n",
    "    classifier = joblib.load('random_forest_model.pkl')\n",
    "    preprocessor = joblib.load('preprocessor.pkl')\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        sentences = file.readlines()\n",
    "\n",
    "    data_list = []\n",
    "    for sentence in sentences:\n",
    "        data_list.append(pd.DataFrame({\n",
    "            'experiences': [sentence],\n",
    "            'Verbs number': [count_verbs(sentence)],\n",
    "            'Adjectives number': [count_adjectives(sentence)],\n",
    "            'Stopwords number': [count_stopwords(sentence)],\n",
    "            'Sentence length': [calculate_sentence_length(sentence)],\n",
    "            'Nouns number': [count_nouns(sentence)],\n",
    "            'Special chars number': [count_special_characters(sentence)],\n",
    "            'Punctuation number': [count_punctuation(sentence)],\n",
    "            'Digits number': [count_digits(sentence)],\n",
    "            'Skills number': [count_skills(skill_extractor, sentence)]\n",
    "        }))\n",
    "\n",
    "    input_df = pd.concat(data_list, ignore_index=True)\n",
    "    X_input = preprocessor.transform(input_df)\n",
    "    predictions = classifier.predict(X_input)\n",
    "    predicted_as_experience = input_df[predictions == 1]\n",
    "\n",
    "    #print(\"\\nüìå Exp√©riences identifi√©es:\")\n",
    "    if predicted_as_experience.empty:\n",
    "        print(\"Aucune phrase d'exp√©rience identifi√©e.\")\n",
    "    else:\n",
    "        for index, row in predicted_as_experience.iterrows():\n",
    "            print(f\"- {row['experiences']}\")\n",
    "\n",
    "    return [s for s, pred in zip(sentences, predictions) if pred == 1]\n",
    "\n",
    "# ========== Exemple d'utilisation ==========\n",
    "if __name__ == \"__main__\":\n",
    "    RESUME_PATH = r\"C:\\Users\\khmir\\Downloads\\khmiri_iheb_tun_eng.pdf\"\n",
    "    txt_path = extract_text_from_pdf(RESUME_PATH)\n",
    "\n",
    "    skills = detectSkills(skill_extractor, txt_path)\n",
    "    experiences = predict(txt_path)\n",
    "    duration = calculate_total_years_experience(txt_path)\n",
    "    countries = detect_address(txt_path)\n",
    "\n",
    "    print(\"\\n========== R√©sultats de l'analyse du CV ==========\")\n",
    "    print(\"Comp√©tences d√©tect√©es:\", \", \".join(skills))\n",
    "    #print(f\"\\nDur√©e totale d'exp√©rience estim√©e: {duration} ans\")\n",
    "    #print(\"\\nPays d√©tect√©s:\", \", \".join(countries) if countries else \"Aucun\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
